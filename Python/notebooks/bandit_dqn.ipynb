{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 16:51:43.945384: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-06-12 16:51:43.949691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib\n",
      "2022-06-12 16:51:43.949703: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "import os\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "import tensorflow as tf  # pylint: disable=g-explicit-tensorflow-version-import\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.bandits.agents.examples.v2 import trainer\n",
    "from tf_agents.bandits.environments import environment_utilities\n",
    "from tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\n",
    "from tf_agents.bandits.metrics import tf_metrics as tf_bandit_metrics\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.utils import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../checkpoints/'\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "CONTEXT_DIM = 15\n",
    "NUM_ACTIONS = 5\n",
    "REWARD_NOISE_VARIANCE = 0.01\n",
    "TRAINING_LOOPS = 400\n",
    "STEPS_PER_LOOP = 2  # DQN agent requires time dim=2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # flags.DEFINE_string('root_dir', os.getenv('TEST_UNDECLARED_OUTPUTS_DIR'),\n",
    "    #                 'Root directory for writing logs/summaries/checkpoints.')\n",
    "\n",
    "    # FLAGS = flags.FLAGS\n",
    "    tf.compat.v1.enable_v2_behavior()  # The trainer only runs with V2 enabled.\n",
    "\n",
    "    with tf.device('/CPU:0'):  # due to b/128333994\n",
    "        action_reward_fns = (\n",
    "            environment_utilities.sliding_linear_reward_fn_generator(\n",
    "                CONTEXT_DIM, NUM_ACTIONS, REWARD_NOISE_VARIANCE))\n",
    "\n",
    "        env = sspe.StationaryStochasticPyEnvironment(\n",
    "            functools.partial(\n",
    "                environment_utilities.context_sampling_fn,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                context_dim=CONTEXT_DIM),\n",
    "            action_reward_fns,\n",
    "            batch_size=BATCH_SIZE)\n",
    "        environment = tf_py_environment.TFPyEnvironment(env)\n",
    "\n",
    "        optimal_reward_fn = functools.partial(\n",
    "            environment_utilities.tf_compute_optimal_reward,\n",
    "            per_action_reward_fns=action_reward_fns)\n",
    "\n",
    "        optimal_action_fn = functools.partial(\n",
    "            environment_utilities.tf_compute_optimal_action,\n",
    "            per_action_reward_fns=action_reward_fns)\n",
    "\n",
    "        q_net = q_network.QNetwork(\n",
    "            environment.observation_spec(),\n",
    "            environment.action_spec(),\n",
    "            fc_layer_params=(50, 50))\n",
    "\n",
    "        agent = dqn_agent.DqnAgent(\n",
    "            environment.time_step_spec(),\n",
    "            environment.action_spec(),\n",
    "            q_network=q_net,\n",
    "            epsilon_greedy=0.1,\n",
    "            target_update_tau=0.05,\n",
    "            target_update_period=5,\n",
    "            optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-2),\n",
    "            td_errors_loss_fn=common.element_wise_squared_loss)\n",
    "\n",
    "        regret_metric = tf_bandit_metrics.RegretMetric(optimal_reward_fn)\n",
    "        suboptimal_arms_metric = tf_bandit_metrics.SuboptimalArmsMetric(\n",
    "            optimal_action_fn)\n",
    "\n",
    "        trainer.train(\n",
    "            root_dir=ROOT_DIR,\n",
    "            agent=agent,\n",
    "            environment=environment,\n",
    "            training_loops=TRAINING_LOOPS,\n",
    "            steps_per_loop=STEPS_PER_LOOP,\n",
    "            additional_metrics=[regret_metric, suboptimal_arms_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3a0aea7473422203c46da3606392391321e1a4ecfa34ed89f71584c047912540"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('rl-fixture')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
