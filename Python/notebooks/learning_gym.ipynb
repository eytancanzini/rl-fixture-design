{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to use Gym, PyTorch and StableBaselines3 for Reinforcement learning\n",
    "Very simple notebook for learning how to use the three tools mentioned above for reinforcement learning. I might even throw in Weights & Biases if I'm feeling lucky and then eventually move onto using MuJoCo for better physics simulation. There is a lot to do here so need to be ready for a lot of fighting\n",
    "## To-Do List\n",
    "- [X] Fix the error where no `torch.tensor` is being pased\n",
    "- [ ] Run the simulation and try and get a decent model\n",
    "- [ ] Evaluate the model on a better set of variables\n",
    "- [ ] Trasnfer to the GPU (somehow? Don't know if CUDA is installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib as plt\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_EPISODES = 20\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "gym.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16037339 -1.0342499   0.21902104  1.6711514 ]\n",
      "Epsiode finished after 45 timesteps\n",
      "[ 0.08830717  0.7962994  -0.2097781  -1.46419   ]\n",
      "Epsiode finished after 26 timesteps\n",
      "[ 0.21185736  1.5983223  -0.22678718 -2.3919704 ]\n",
      "Epsiode finished after 22 timesteps\n",
      "[-0.01129861 -0.0545598   0.22464773  1.1823016 ]\n",
      "Epsiode finished after 30 timesteps\n",
      "[ 0.13709576  0.37880445 -0.23063438 -0.9599621 ]\n",
      "Epsiode finished after 20 timesteps\n",
      "[-0.08154249  0.00979014  0.22887889  0.7721656 ]\n",
      "Epsiode finished after 32 timesteps\n",
      "[-0.17901531 -1.3363675   0.2388248   2.3129249 ]\n",
      "Epsiode finished after 13 timesteps\n",
      "[-0.14487733 -0.6426741   0.21945454  1.2984225 ]\n",
      "Epsiode finished after 17 timesteps\n",
      "[ 0.08514409  0.18760517 -0.21312739 -0.70261735]\n",
      "Epsiode finished after 15 timesteps\n",
      "[ 0.01621801  0.76733685 -0.21554087 -1.5854744 ]\n",
      "Epsiode finished after 32 timesteps\n",
      "[ 0.04137275 -0.551702   -0.21166815 -0.12708661]\n",
      "Epsiode finished after 27 timesteps\n",
      "[ 0.09635995  1.3669235  -0.2264291  -2.332564  ]\n",
      "Epsiode finished after 11 timesteps\n",
      "[ 0.07415914  1.1549938  -0.21102197 -2.0524864 ]\n",
      "Epsiode finished after 16 timesteps\n",
      "[-0.02741791  0.20222566 -0.2110738  -1.0296745 ]\n",
      "Epsiode finished after 35 timesteps\n",
      "[ 0.06313229  0.05936516 -0.22279137 -0.80127794]\n",
      "Epsiode finished after 36 timesteps\n",
      "[-0.05655434 -1.1565672   0.22358128  2.0205534 ]\n",
      "Epsiode finished after 12 timesteps\n",
      "[ 0.15846165  0.80110925 -0.21290495 -1.4196162 ]\n",
      "Epsiode finished after 24 timesteps\n",
      "[-0.18629302 -1.5407635   0.21224238  2.3703866 ]\n",
      "Epsiode finished after 16 timesteps\n",
      "[ 0.12091474  0.9828036  -0.21601737 -1.6471943 ]\n",
      "Epsiode finished after 13 timesteps\n",
      "[ 0.17660911  1.3985988  -0.24520746 -2.281517  ]\n",
      "Epsiode finished after 11 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Start with just the simple cartpole problem\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "for idx in range(MAX_EPISODES):\n",
    "    observation = env.reset()\n",
    "    for t in range(MAX_ITERATIONS):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(observation)\n",
    "            print(f\"Epsiode finished after {t+1} timesteps\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Information regarding environment:\n",
    "\n",
    "Observation space:\n",
    "(4,) array with elements: [position, velocity, angle, angular velocity]\n",
    "\n",
    "Action space:\n",
    "(1,) array that is in the range {0,1} (DISCRETE)\n",
    "\"\"\"\n",
    "\n",
    "import math \n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to convert the work in the \"Hands-On ML\" book to work in PyTorch. I'd rather work in PyTorch simply for my own work. For deep Q-learning, we get a function $Q^*:\\textit{State}\\times\\textit{Action}\\rightarrow \\mathbb{R}$ which gives us the return for a specific action in a state. We want to maximise this: $\\pi^*(s)=\\underset{a}{\\mbox{argmax }}Q^*(s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 4 # Input shape is the observations of the cartpole [pos, vel, ang, ang_vel]\n",
    "output_shape = 2 # Output shape is the action space size {-1,1}\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Replay buffer used to store the previous steps taken in the training algorithm. Uses the deque function.\n",
    "    (Could change to using the Reverb library from DeepMind)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (l_input): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (hidden_1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (l_output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a DQN model in PyTorch. This is done using a class function to create a model parameters\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Very basic network that has all linear layers with an input of 4 (the observations) and an output of 2 (the actions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs, output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.l_input = nn.Linear(inputs, 32)\n",
    "        self.hidden_1 = nn.Linear(32, 32)\n",
    "        self.l_output = nn.Linear(32, output)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l_input(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_1(x)\n",
    "        x = F.relu(x)\n",
    "        return self.l_output(x)\n",
    "    \n",
    "net = DQN(input_shape, output_shape)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (l_input): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (hidden_1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (l_output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN(input_shape, output_shape).to(device)\n",
    "target_net = DQN(input_shape, output_shape).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part, we need to setup the optimiser for our model along with the function that determines taking a new step in the next direction. These functions will be adapted from the section in the \"Hands-On ML\" book, but using PyTorch for better future proofing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) *  math.exp(-1*steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(0)[1].view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(output_shape)]], device=device, dtype=torch.long)\n",
    "    \n",
    "def optimise_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) for the model. \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all the next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "    # Step the optimiser\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step is the actual training loop. This is the part that causes problems, so need to be careful how we do this. The observation comes in the form of a `nparray` whereas we want it in the form of a `torch.tensor`. This conversion has to be done before the model is called as otherwise it doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [00:00<00:00, 5011.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 13/200 [00:00<00:00, 2959.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 24/200 [00:00<00:00, 5726.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:00<00:00, 4599.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Number: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [00:00<00:00, 5162.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 5 # DOES NOT WORK AT NUMBERS GREATER THAN THIS\n",
    "NUM_ITERATIONS = 200\n",
    "\n",
    "episode_durations = []\n",
    "steps_done = 0\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "Need to find out why it is not a torch tensor. Do I need to convert everything to tensors to stop the error?\n",
    "UPDATE 25/05/2022@17:00 - It seems I do need to change everything to tensors to get the error to go away, but it seems that the memory isn't clearing\n",
    "\"\"\"\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    \n",
    "    print(f\"Episode Number: {i_episode}\")\n",
    "    \n",
    "    # Reset the env at the beginning of the episode\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for idx in tqdm(range(NUM_ITERATIONS)):\n",
    "        # Convert the state and get the action\n",
    "        state = torch.from_numpy(obs)\n",
    "        action = select_action(state)\n",
    "        \n",
    "        # Step the environment with the chosen action\n",
    "        state_obs, reward, done, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # Check to see if the env is done or not\n",
    "        if not done:\n",
    "            next_state = torch.from_numpy(state_obs)\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        # Add this information to the buffer\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        # Move onto the next state and optimise the model\n",
    "        obs = state_obs\n",
    "        optimise_model()\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(idx + 1)\n",
    "            break;\n",
    "    if i_episode & TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "print(\"Finished training\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0768502   0.79675883 -0.21398349 -1.4539504 ]\n",
      "Epsiode finished after 16 timesteps\n",
      "[ 0.15789364  1.136877   -0.24816257 -2.0722616 ]\n",
      "Epsiode finished after 16 timesteps\n",
      "[ 0.1826065   1.3842472  -0.22382887 -2.2761858 ]\n",
      "Epsiode finished after 17 timesteps\n",
      "[ 0.2268194   1.7928833  -0.23885089 -2.757588  ]\n",
      "Epsiode finished after 15 timesteps\n",
      "[ 0.172525    1.5639651  -0.22253878 -2.5371559 ]\n",
      "Epsiode finished after 10 timesteps\n",
      "[ 0.17375931  1.1689367  -0.21678406 -2.0024483 ]\n",
      "Epsiode finished after 12 timesteps\n",
      "[ 0.20481385  1.7628695  -0.23886864 -2.7398944 ]\n",
      "Epsiode finished after 13 timesteps\n",
      "[ 0.12859298  1.560231   -0.21214361 -2.4135008 ]\n",
      "Epsiode finished after 12 timesteps\n",
      "[ 0.17984591  2.097453   -0.25302044 -3.2621856 ]\n",
      "Epsiode finished after 13 timesteps\n",
      "[ 0.15543552  1.3692449  -0.21164376 -2.229436  ]\n",
      "Epsiode finished after 9 timesteps\n",
      "[ 0.10136188  1.8122773  -0.24113558 -2.826864  ]\n",
      "Epsiode finished after 9 timesteps\n",
      "[ 0.13333464  0.9483672  -0.23620047 -1.8014073 ]\n",
      "Epsiode finished after 13 timesteps\n",
      "[ 0.10143049  1.5361134  -0.22141042 -2.5411148 ]\n",
      "Epsiode finished after 8 timesteps\n",
      "[ 0.15438262  1.8073838  -0.2531927  -2.8856452 ]\n",
      "Epsiode finished after 9 timesteps\n",
      "[ 0.18646847  1.9979424  -0.24016726 -3.0225112 ]\n",
      "Epsiode finished after 10 timesteps\n",
      "[ 0.22131592  1.6142641  -0.25266454 -2.5491526 ]\n",
      "Epsiode finished after 12 timesteps\n",
      "[ 0.14635049  1.9871179  -0.2430149  -3.0829294 ]\n",
      "Epsiode finished after 10 timesteps\n",
      "[ 0.12623663  0.8013255  -0.23386657 -1.3975338 ]\n",
      "Epsiode finished after 10 timesteps\n",
      "[ 0.226392    1.7454265  -0.21591517 -2.6923048 ]\n",
      "Epsiode finished after 11 timesteps\n",
      "[ 0.16326417  1.3828337  -0.23400998 -2.1405733 ]\n",
      "Epsiode finished after 11 timesteps\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for idx in range(MAX_EPISODES):\n",
    "    obs = env.reset()\n",
    "    for t in range(MAX_ITERATIONS):\n",
    "        env.render()\n",
    "        state = torch.from_numpy(obs)\n",
    "        action = select_action(state)\n",
    "        observation, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        if done:\n",
    "            print(observation)\n",
    "            print(f\"Epsiode finished after {t+1} timesteps\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ReplayMemory at 0x7fadce6feac0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions = memory.sample(5)\n",
    "batch = Transition(*zip(*transitions))\n",
    "non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                        batch.next_state)), device=device, dtype=torch.bool)\n",
    "non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "state_batch = torch.cat(batch.state)\n",
    "action_batch = torch.cat(batch.action)\n",
    "reward = torch.cat(batch.reward)\n",
    "action_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x20 and 4x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, action_batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-fixture/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_1(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-fixture/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/rl-fixture/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x20 and 4x32)"
     ]
    }
   ],
   "source": [
    "policy_net(state_batch).gather(1, action_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97b2bf48e32d8e2de32d844e69cf674f1390fa5d8c53747b94ee65ed539bc7c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
