{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to use Gym, PyTorch and StableBaselines3 for Reinforcement learning\n",
    "Very simple notebook for learning how to use the three tools mentioned above for reinforcement learning. I might even throw in Weights & Biases if I'm feeling lucky and then eventually move onto using MuJoCo for better physics simulation. There is a lot to do here so need to be ready for a lot of fighting\n",
    "## To-Do List\n",
    "- [ ] Fix the error where no `torch.tensor` is being pased\n",
    "- [ ] Run the simulation and try and get a decent model\n",
    "- [ ] Evaluate the model on a better set of variables\n",
    "- [ ] Trasnfer to the GPU (somehow? Don't know if CUDA is installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib as plt\n",
    "from tqdm import tqdm\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "MAX_EPISODES = 20\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "gym.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03778828  0.21992627 -0.21858265 -0.99981505]\n",
      "Epsiode finished after 35 timesteps\n",
      "[-0.13366812 -0.41815755  0.2115568   1.1740344 ]\n",
      "Epsiode finished after 30 timesteps\n",
      "[-0.19203323 -0.5585572   0.21386503  1.1523181 ]\n",
      "Epsiode finished after 13 timesteps\n",
      "[-0.07295746  0.06602277 -0.22327465 -1.3058242 ]\n",
      "Epsiode finished after 42 timesteps\n",
      "[-0.07880972 -0.97056675  0.2420936   1.7659268 ]\n",
      "Epsiode finished after 9 timesteps\n",
      "[ 0.2104759  1.7434605 -0.2310046 -2.7885432]\n",
      "Epsiode finished after 13 timesteps\n",
      "[-0.12281333 -0.9711392   0.21947926  1.7758803 ]\n",
      "Epsiode finished after 15 timesteps\n",
      "[-0.05352812 -0.40221405  0.2128541   1.0717787 ]\n",
      "Epsiode finished after 18 timesteps\n",
      "[-0.13433659 -0.3704068   0.21527629  1.0174774 ]\n",
      "Epsiode finished after 14 timesteps\n",
      "[ 0.13332753  0.22601157 -0.228375   -0.8359939 ]\n",
      "Epsiode finished after 25 timesteps\n",
      "[-0.09054015 -0.7553634   0.2110363   1.6617942 ]\n",
      "Epsiode finished after 16 timesteps\n",
      "[ 0.10031619 -0.02151084 -0.21141703 -0.5745544 ]\n",
      "Epsiode finished after 18 timesteps\n",
      "[-0.02514747 -1.3365043   0.23660402  2.3607254 ]\n",
      "Epsiode finished after 61 timesteps\n",
      "[ 0.15868334  1.2034132  -0.24039833 -2.066974  ]\n",
      "Epsiode finished after 10 timesteps\n",
      "[-0.08312049 -0.77796805  0.21165429  1.4155866 ]\n",
      "Epsiode finished after 8 timesteps\n",
      "[-0.13566574 -1.1638454   0.21110013  1.9823545 ]\n",
      "Epsiode finished after 12 timesteps\n",
      "[ 0.147891    1.3354218  -0.22316502 -2.3174639 ]\n",
      "Epsiode finished after 15 timesteps\n",
      "[ 0.1637011   0.579518   -0.21679473 -0.8009041 ]\n",
      "Epsiode finished after 63 timesteps\n",
      "[-0.02736151  0.8421712  -0.21792626 -2.1268222 ]\n",
      "Epsiode finished after 36 timesteps\n",
      "[-0.09849325 -0.8055169   0.21308829  1.443144  ]\n",
      "Epsiode finished after 14 timesteps\n"
     ]
    }
   ],
   "source": [
    "# Start with just the simple cartpole problem\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "for idx in range(MAX_EPISODES):\n",
    "    observation = env.reset()\n",
    "    for t in range(MAX_ITERATIONS):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(observation)\n",
    "            print(f\"Epsiode finished after {t+1} timesteps\")\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/miniconda3/envs/rl-fixture/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Information regarding environment:\n",
    "\n",
    "Observation space:\n",
    "(4,) array with elements: [position, velocity, angle, angular velocity]\n",
    "\n",
    "Action space:\n",
    "(1,) array that is in the range {0,1} (DISCRETE)\n",
    "\"\"\"\n",
    "\n",
    "import math \n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to convert the work in the \"Hands-On ML\" book to work in PyTorch. I'd rather work in PyTorch simply for my own work. For deep Q-learning, we get a function $Q^*:\\textit{State}\\times\\textit{Action}\\rightarrow \\mathbb{R}$ which gives us the return for a specific action in a state. We want to maximise this: $\\pi^*(s)=\\underset{a}{\\mbox{argmax }}Q^*(s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = 4 # Input shape is the observations of the cartpole [pos, vel, ang, ang_vel]\n",
    "output_shape = 2 # Output shape is the action space size {-1,1}\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \"\"\"\n",
    "    Replay buffer used to store the previous steps taken in the training algorithm. Uses the deque function.\n",
    "    (Could change to using the Reverb library from DeepMind)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (l_input): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (hidden_1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (l_output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a DQN model in PyTorch. This is done using a class function to create a model parameters\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Very basic network that has all linear layers with an input of 4 (the observations) and an output of 2 (the actions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inputs, output):\n",
    "        super(DQN, self).__init__()\n",
    "        self.l_input = nn.Linear(inputs, 32)\n",
    "        self.hidden_1 = nn.Linear(32, 32)\n",
    "        self.l_output = nn.Linear(32, output)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l_input(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden_1(x)\n",
    "        x = F.relu(x)\n",
    "        return self.l_output(x)\n",
    "    \n",
    "net = DQN(input_shape, output_shape)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (l_input): Linear(in_features=4, out_features=32, bias=True)\n",
       "  (hidden_1): Linear(in_features=32, out_features=32, bias=True)\n",
       "  (l_output): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN(input_shape, output_shape).to(device)\n",
    "target_net = DQN(input_shape, output_shape).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next part, we need to setup the optimiser for our model along with the function that determines taking a new step in the next direction. These functions will be adapted from the section in the \"Hands-On ML\" book, but using PyTorch for better future proofing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) *  math.exp(-1*steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(0)[1].view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(output_shape)]], device=device, dtype=torch.long)\n",
    "    \n",
    "def optimise_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward = torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a) for the model. \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all the next states\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    # Compute the Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "    # Step the optimiser\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step is the actual training loop. This is the part that causes problems, so need to be careful how we do this. The observation comes in the form of a `nparray` whereas we want it in the form of a `torch.tensor`. This conversion has to be done before the model is called as otherwise it doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Move onto the next state and optimise the model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m obs \u001b[38;5;241m=\u001b[39m state\n\u001b[0;32m---> 33\u001b[0m \u001b[43moptimise_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     36\u001b[0m     episode_durations\u001b[38;5;241m.\u001b[39mappend(idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36moptimise_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m batch \u001b[38;5;241m=\u001b[39m Transition(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mtransitions))\n\u001b[1;32m     17\u001b[0m non_final_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m s: s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                                         batch\u001b[38;5;241m.\u001b[39mnext_state)), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[0;32m---> 19\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m state_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39mstate)\n\u001b[1;32m     22\u001b[0m action_batch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(batch\u001b[38;5;241m.\u001b[39maction)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 5\n",
    "NUM_ITERATIONS = 200\n",
    "\n",
    "\"\"\"\n",
    "TODO:\n",
    "Need to find out why it is not a torch tensor. Do I need to convert everything to tensors to stop the error?\n",
    "UPDATE 25/05/2022@17:00 - It seems I do need to change everything to tensors to get the error to go away, but it seems that the memory isn't clearing\n",
    "\"\"\"\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    \n",
    "    # Reset the env at the beginning of the episode\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for idx in tqdm(range(NUM_ITERATIONS)):\n",
    "        # Convert the state and get the action\n",
    "        state = torch.tensor(obs)\n",
    "        action = select_action(state)\n",
    "        \n",
    "        # Step the environment with the chosen action\n",
    "        obs, reward, done, info = env.step(action.item())\n",
    "        \n",
    "        # Check to see if the env is done or not\n",
    "        if not done:\n",
    "            next_state = obs\n",
    "        else:\n",
    "            next_state = None\n",
    "        \n",
    "        # Add this information to the buffer\n",
    "        memory.push(state, action, torch.tensor(next_state), torch.tensor(reward))\n",
    "        \n",
    "        # Move onto the next state and optimise the model\n",
    "        obs = state\n",
    "        optimise_model()\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(idx + 1)\n",
    "            break;\n",
    "    if i_episode & TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "print(\"Finished training\")             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97b2bf48e32d8e2de32d844e69cf674f1390fa5d8c53747b94ee65ed539bc7c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
